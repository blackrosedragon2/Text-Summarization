{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSummarization",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackrosedragon2/Text-Summarization/blob/master/TextSummarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEvQFi0wa1fA",
        "colab_type": "code",
        "outputId": "dc69ba8d-2fbd-4693-e596-8b2c0c3e49b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "!pip install tqdm\n",
        "!pip install pyspellchecker\n",
        "!pip install torchsummaryx"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.6/dist-packages (0.5.2)\n",
            "Requirement already satisfied: torchsummaryx in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchsummaryx) (1.17.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from torchsummaryx) (0.25.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchsummaryx) (1.3.0+cu100)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryx) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->torchsummaryx) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->torchsummaryx) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqrMHmThPzJb",
        "colab_type": "code",
        "outputId": "bc897703-5f01-4f7b-b3de-bf6f3da8bc07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Reading data onto zipfile object ZF\n",
        "from google.colab import drive\n",
        "import os \n",
        "import zipfile\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = '/content/gdrive/My Drive/data/'  \n",
        "zf=zipfile.ZipFile(root_path+\"cnn_stories.zip\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9aAAGcDdq2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from spellchecker import SpellChecker\n",
        "class preprocess:\n",
        "    #Accepts a string and performs string operations on it\n",
        "    def seperate(self,para):#1\n",
        "        # find @highlight, add length and split story and summary, then decode to utf-8 format, use split tokenizer and seperate the 4 lines , split the \\n parts and return the list\n",
        "        story=para[:para.find('@highlight')]\n",
        "        summary=[i.splitlines()[2] for i in (para[para.find('@highlight')+len('@highlight'):]).split('@highlight')]\n",
        "        return story,summary    \n",
        "        #returns (string,list)\n",
        "    \n",
        "    def spell_check(self,para):#2\n",
        "        #corrects spellings(may not be used for us) + takes much longer to correct spellings (5+ minutes)\n",
        "        spell = SpellChecker()\n",
        "        para=para.split(' ')\n",
        "        correctpara=[]\n",
        "        for word in para:\n",
        "            correctpara.append(spell.correction(word))\n",
        "        return ' '.join(correctpara)\n",
        "\n",
        "    def remove_apostrophes(self,para):#3\n",
        "        return re.sub(\"(?<=[a-z])'(?=[a-z])\", \"\", para)\n",
        "\n",
        "    def remove_parentheses(self,para):#4\n",
        "        #removes anything inside parentheses\n",
        "        return re.sub(r'\\([^)]*\\)', '', para)\n",
        "\n",
        "    def remove_sp(self,para):#5\n",
        "        #removes special symbols\n",
        "        return re.sub('[^A-Za-z0-9]+', ' ', para)   #note: add exception for words like U.N. \n",
        "\n",
        "    def remove_short(self,para):#6\n",
        "        #removes words that are <3 in length\n",
        "        para=para.split(' ')\n",
        "        longpara=[]\n",
        "        for word in para:\n",
        "            if len(word) >= 3:\n",
        "                longpara.append(word)\n",
        "        return ' '.join(longpara)\n",
        "\n",
        "    def add_tags(self,para):#7\n",
        "        #add start and end tags \n",
        "        para=\"<start>\"+para+\"<end>\"\n",
        "        return para\n",
        "\n",
        "    def apply_all(self,para):#8\n",
        "        #applies 1,3,4,5,6 functions, may need reordering \n",
        "        p=preprocess()\n",
        "\n",
        "        story,summary=p.seperate(para.decode(\"utf-8\"))\n",
        "\n",
        "        #story=p.spell_check(story)\n",
        "        #summary=p.spell_check(summary)\n",
        "\n",
        "        story=p.remove_apostrophes(story)\n",
        "        summary=[p.remove_apostrophes(i) for i in summary] \n",
        "\n",
        "        story=p.remove_parentheses(story)\n",
        "        summary=[p.remove_parentheses(i) for i in summary]\n",
        "\n",
        "        story=p.remove_sp(story)\n",
        "        summary=[p.remove_sp(i) for i in summary]\n",
        "\n",
        "        story=p.remove_short(story)\n",
        "        summary=[p.remove_short(i) for i in summary]\n",
        "\n",
        "        return story,summary\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRFIPzWhP-Km",
        "colab_type": "code",
        "outputId": "9a7486a8-22f4-4c36-8afc-bdc757c65ae6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "#Read content of all files onto x (takes approximatly 13 minutes for both modes)\n",
        "from tqdm import tqdm\n",
        "\n",
        "def get_data(preproc=0):\n",
        "    i=1\n",
        "    if preproc==1:\n",
        "        stories=[]\n",
        "        story=[]\n",
        "        summaries=[]\n",
        "        summary=[]\n",
        "        p=preprocess()\n",
        "        with tqdm(total=92600) as pbar:\n",
        "            while(True):\n",
        "                #Reads zip file until eof exception occurs\n",
        "                i=i+1\n",
        "                try:\n",
        "                    story,summary=p.apply_all(zf.open(zf.namelist()[i]).read())\n",
        "                    stories.append(story)\n",
        "                    summaries.append(summary)\n",
        "                except Exception as E:\n",
        "                    break\n",
        "                pbar.update(1)\n",
        "        return stories,summaries\n",
        "    else:\n",
        "        x=[]\n",
        "        with tqdm(total=92600) as pbar:\n",
        "            while(True):\n",
        "                #Reads zip file until eof exception occurs\n",
        "                i=i+1\n",
        "                try:\n",
        "                    x.append(zf.open(zf.namelist()[i]).read())\n",
        "                except Exception as E:\n",
        "                    break\n",
        "                pbar.update(1)\n",
        "        return x\n",
        "\n",
        "_,summa=get_data(1) \n",
        "for j,i in enumerate(summa):\n",
        "    print(i)\n",
        "    if j>10:\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 92579/92600 [09:25<00:00, 163.81it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['Syrian official Obama climbed the top the tree doesnt know how get down', 'Obama sends letter the heads the House and Senate', 'Obama seek congressional approval military action against Syria', 'Aim determine whether were used not whom says spokesman']\n",
            "['Usain Bolt wins third gold world championship', 'Anchors Jamaica 4x100m relay victory', 'Eighth gold the championships for Bolt', 'Jamaica double womens 4x100m relay']\n",
            "['The employee agencys Kansas City office among hundreds virtual workers', 'The employees travel and from the mainland last year cost more than 000', 'The telecommuting program like all GSA practices under review']\n",
            "['NEW Canadian doctor says she was part team examining Harry Burkhart 2010', 'NEW Diagnosis autism severe anxiety post traumatic stress disorder and depression', 'Burkhart also suspected German arson probe officials say', 'Prosecutors believe the German national set string fires Los Angeles']\n",
            "['Another arrest made gang rape outside California school', 'Investigators say people took part stood and watched the assault', 'Four suspects appeared court Thursday three wore bulletproof vests']\n",
            "['Humanitarian groups expect 000 refugees one camp official says', 'Others have fled across the border camps Liberia says', 'This follows attacks that killed peacekeepers and civilians']\n",
            "['NEW groups announce legal challenge Phoenix', 'American Civil Liberties Union ACLU Arizona National Immigration Law Center slam law', 'Mexican American Legal Defense and Educational Fund also objects', 'They say law encourages racial profiling but supporters say doesnt involve any illegal acts']\n",
            "['Labor Day the unofficial end summer and the unofficial start campaign season', 'much billion could spent advertising for this midterm election', 'Here are five must follow races for these midterms']\n",
            "['NEW Autopsy indicates had been dead for hours before police arrived', 'YouTube video appears show the activist bound and blindfolded', 'The activist and freelance journalist was from the Lombardy region northern Italy']\n",
            "['The radio personality was taken hospital Sunday', 'Spokesman for hospital says Casey Kasem being treated for wounds blood pressure issues', 'had been friends home Washington state after his wife took him there', 'She has been feuding with three stepchildren over the radio icons care']\n",
            "['Hawaiian Airlines again lands time performance', 'The Airline Quality Rankings Report looks the largest airlines', 'ExpressJet and American Airlines had the worst time performance', 'Virgin America had the best baggage handling Southwest had lowest complaint rate']\n",
            "['The new cardinals will installed February', 'They come from countries such Myanmar and Tonga', 'Americans made the list this time the previous time Francis papacy']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XalfZdPbhJ1M",
        "colab_type": "code",
        "outputId": "fb7759fa-72b1-40c7-8435-aaf077e25228",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "%xmode Plain\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "class GRU_Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop=0.2):\n",
        "        super(GRU_Encoder, self).__init__()\n",
        "        #GRU with dropout,fc layer, relu layer\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers*2 # bidirectional\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
        "        self.gru = nn.GRU(hidden_dim, hidden_dim, n_layers, batch_first=True, dropout=drop, bidirectional =True) \n",
        "        #batch first will create dims such that dimensions=(batch,dim1,dim2...)\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data      \n",
        "         #grabs any parameter of the model and uses it to instantiate (through .data.new) a new tensor on the same device\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        #ints a hidden vector shape( n_layer,batch_size,hidden_dim) to zero\n",
        "        return hidden\n",
        "\n",
        "    def forward(self, x, h=0):# x dims=()\n",
        "        if h==0:\n",
        "            h=self.init_hidden(32)\n",
        "\n",
        "        embedded = self.embedding(x).view(x.shape[0],-1,self.hidden_dim)\n",
        "        out=embedded\n",
        "        out, h = self.gru(out, h)  \n",
        "        print(\"X {},OUTPUT {},HIDDEN {}\".format(x.shape,out.shape,h.shape))\n",
        "\n",
        "        return out, h  #(output of encoder(give to-> attention mechanism ), next hidden state)\n",
        "\n",
        "#input trensor dims must be (i/p dim,timesteps,batchsize)\n",
        "gruEncoder_model=GRU_Encoder(input_dim=100, hidden_dim=128, output_dim=20, n_layers=10)\n",
        "inputs = torch.zeros((32,20, 100), dtype=torch.long) # (batch_size, outputdim, inputdim)\n",
        "inputs=inputs.cuda()\n",
        "gruEncoder_model.cuda()\n",
        "summary(gruEncoder_model,inputs)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exception reporting mode: Plain\n",
            "X torch.Size([32, 20, 100]),OUTPUT torch.Size([32, 2000, 256]),HIDDEN torch.Size([20, 32, 128])\n",
            "================================================================\n",
            "            Kernel Shape        Output Shape   Params  Mult-Adds\n",
            "Layer                                                           \n",
            "0_embedding   [128, 100]  [32, 20, 100, 128]    12800      12800\n",
            "1_gru                  -     [32, 2000, 256]  2866176    2850816\n",
            "----------------------------------------------------------------\n",
            "                       Totals\n",
            "Total params          2878976\n",
            "Trainable params      2878976\n",
            "Non-trainable params        0\n",
            "Mult-Adds             2863616\n",
            "================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding</th>\n",
              "      <td>[128, 100]</td>\n",
              "      <td>[32, 20, 100, 128]</td>\n",
              "      <td>12800</td>\n",
              "      <td>12800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_gru</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 2000, 256]</td>\n",
              "      <td>2866176</td>\n",
              "      <td>2850816</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Kernel Shape        Output Shape   Params  Mult-Adds\n",
              "Layer                                                           \n",
              "0_embedding   [128, 100]  [32, 20, 100, 128]    12800      12800\n",
              "1_gru                  -     [32, 2000, 256]  2866176    2850816"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4qXNtls1iux",
        "colab_type": "code",
        "outputId": "3200cd7f-7ac4-4ca1-e911-98edd87f6b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        }
      },
      "source": [
        "'''class GRU_AttnDecoder(nn.Module):\n",
        "    def __init__(self, hidden_dim, output_dim,n_layers, dropout = 0.2, max_length = 15):\n",
        "        super(GRU_AttnDecoder, self).__init__()\n",
        "        self.n_layers=n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.dropout = dropout\n",
        "        self.max_length = max_length\n",
        "        \n",
        "        self.embedding = nn.Embedding(self.output_dim, self.hidden_dim)\n",
        "        self.attn = nn.Linear(self.hidden_dim * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_dim * 2, self.hidden_dim)\n",
        "        self.gru = nn.GRU(self.hidden_dim, self.hidden_dim,n_layers, batch_first=True, dropout=dropout, bidirectional =False)\n",
        "        self.out = nn.Linear(self.hidden_dim, self.output_dim)\n",
        "        \n",
        "    def forward(self, input, hidden=1):\n",
        "        if hidden==0:\n",
        "            hidden=self.initHidden()\n",
        "\n",
        "        embedded = self.embedding(input).view(1,1,-1)\n",
        "        \n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0]),1)),dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0), encoder_outputs.unsqueeze(0)) \n",
        "        #torch.bmm performs batch matrix multiplication\n",
        "        \n",
        "        output = torch.cat((embedded[0], attn_applied[0]),1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        \n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        \n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "    \n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1,1,self.hidden_dim, device=device)'''\n",
        "\n",
        "class GRU_AttentionDecoder(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size,n_layers,dropout=0.2):\n",
        "        super(GRU_AttentionDecoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers= n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size,n_layers,batch_first=True, dropout=dropout, bidirectional =False) \n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def forward(self, input, hidden=1):\n",
        "        if hidden==1:\n",
        "            hidden=self.initHidden(32)\n",
        "        output = self.embedding(input).view(input.shape[0],-1,self.hidden_size)\n",
        "        print(\"INPUT {},OUTPUT {},HIDDEN {}\".format(input.shape,output.shape,hidden.shape))\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data      \n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_size).zero_().to(device)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "decoder_model=GRU_AttentionDecoder( hidden_size=128, output_size=20,n_layers=10)\n",
        "inputs = torch.zeros((32,20,128), dtype=torch.long) # (batch_size, outputdim, inputdim)\n",
        "inputs=inputs.cuda()\n",
        "decoder_model.cuda()\n",
        "summary(decoder_model,inputs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INPUT torch.Size([32, 20, 128]),OUTPUT torch.Size([32, 2560, 128]),HIDDEN torch.Size([10, 32, 128])\n",
            "=================================================================\n",
            "            Kernel Shape        Output Shape   Params Mult-Adds\n",
            "Layer                                                          \n",
            "0_embedding    [128, 20]  [32, 20, 128, 128]    2.56k     2.56k\n",
            "1_gru                  -     [32, 2560, 128]  990.72k   983.04k\n",
            "2_out          [128, 20]          [2560, 20]    2.58k     2.56k\n",
            "3_softmax              -          [2560, 20]        -         -\n",
            "-----------------------------------------------------------------\n",
            "                       Totals\n",
            "Total params          995.86k\n",
            "Trainable params      995.86k\n",
            "Non-trainable params      0.0\n",
            "Mult-Adds             988.16k\n",
            "=================================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_embedding</th>\n",
              "      <td>[128, 20]</td>\n",
              "      <td>[32, 20, 128, 128]</td>\n",
              "      <td>2560.0</td>\n",
              "      <td>2560.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_gru</th>\n",
              "      <td>-</td>\n",
              "      <td>[32, 2560, 128]</td>\n",
              "      <td>990720.0</td>\n",
              "      <td>983040.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_out</th>\n",
              "      <td>[128, 20]</td>\n",
              "      <td>[2560, 20]</td>\n",
              "      <td>2580.0</td>\n",
              "      <td>2560.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[2560, 20]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Kernel Shape        Output Shape    Params  Mult-Adds\n",
              "Layer                                                            \n",
              "0_embedding    [128, 20]  [32, 20, 128, 128]    2560.0     2560.0\n",
              "1_gru                  -     [32, 2560, 128]  990720.0   983040.0\n",
              "2_out          [128, 20]          [2560, 20]    2580.0     2560.0\n",
              "3_softmax              -          [2560, 20]       NaN        NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwF1bV1NLgdS",
        "colab_type": "code",
        "outputId": "b7de9708-32c5-425a-9e7d-0af202bde5e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "32*20*128"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81920"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    }
  ]
}